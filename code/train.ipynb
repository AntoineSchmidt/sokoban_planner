{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "from astar import *\n",
    "from replay import *\n",
    "from network import *\n",
    "from generator import *\n",
    "\n",
    "from manage import *\n",
    "startSession()\n",
    "\n",
    "logs_base_dir = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://localhost:6006/#scalars\" target=\"_blank\">TensorBoard</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train Sokoban environments\n",
    "\n",
    "sokoban_train = Sokoban(count=10000)\n",
    "with open('data/sokoban_train.pkl', 'wb') as f:\n",
    "    pickle.dump(sokoban_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load generated train environments\n",
    "# upgrade numpy to > 1.17 when problems loading\n",
    "\n",
    "with open('data/sokoban_train.pkl', 'rb') as f:\n",
    "    sokoban_train = pickle.load(f)\n",
    "split = int(0.1 * len(sokoban_train.all_envs)) # validation, train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse ff / improved environment solutions\n",
    "\n",
    "if False:\n",
    "    improved_train = []\n",
    "    improved_validation = []\n",
    "    for i in tqdm(range(split, len(sokoban_train.all_envs))):\n",
    "        env, plan, _ = sokoban_train.get(i)\n",
    "        plan_ff = solve(env)\n",
    "        if plan_ff: ratio = len(plan) / len(plan_ff)\n",
    "        else: continue\n",
    "\n",
    "        if ratio == 1: # search optimal solution\n",
    "            plan_opt = search_astar(env, cutoff=10000)\n",
    "            if plan_opt:\n",
    "                ratio_optimal = len(plan_opt) / len(plan_ff)\n",
    "                if 0 < ratio_optimal < 1: print(i, ratio_optimal)\n",
    "\n",
    "        if ratio < 1: # improved by policy\n",
    "            if i < split: improved_validation += [ratio]\n",
    "            else: improved_train += [ratio]\n",
    "\n",
    "    print(len(improved_validation) / split, len(improved_train) / (len(sokoban_train.all_envs) - split)) # 0.372 0.389\n",
    "\n",
    "\n",
    "s = [11, 113, 141, 152, 223, 356, 465, 580, 855] # improved validation envs\n",
    "s = [1203, 1834, 3603, 3124, 5906, 7009, 7293, 7531, 7660, 7830, 8080, 8276, 8372, 9479] # improved train envs\n",
    "s = [65, 1017, 1023, 1024] # bad unimproved solutions\n",
    "\n",
    "for i in s:\n",
    "    env, plan, _ = sokoban_train.get(i)\n",
    "    plan_ff = solve(env)\n",
    "    plan_as = search_astar(env, cutoff=10000)\n",
    "\n",
    "    print(i, len(plan), len(plan) / len(plan_ff))\n",
    "    print(i, len(plan_as), len(plan_as) / len(plan_ff))\n",
    "\n",
    "    print([list(action_dict.keys())[action] for action in plan])\n",
    "    print([list(action_dict.keys())[action] for action in plan_ff])\n",
    "    print([list(action_dict.keys())[action] for action in plan_as])\n",
    "    draw(env, 'images/sol_{}.png'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imitation Learning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve ff training solution pathes using learned policy\n",
    "\n",
    "model, _ = create()\n",
    "model.load_weights('models/il_action.h5')\n",
    "\n",
    "improved = 0\n",
    "for i in tqdm(range(len(sokoban_train.all_envs))):\n",
    "    plan_learned = []\n",
    "    env, plan, _ = sokoban_train.get(i)\n",
    "\n",
    "    for _ in range(len(plan) - 1):\n",
    "        env_b = binary(strip(env))[0]\n",
    "        action = np.argmax(model.predict(np.expand_dims(env_b, axis=0))[0]) # best predicted action\n",
    "        plan_learned.append(action)\n",
    "        if env.step(action_translator[action], 'tiny_rgb_array')[2]: # better solution found\n",
    "            improved += 1\n",
    "            print(i, len(plan_learned) / len(plan))\n",
    "            sokoban_train.all_envs[i] = (sokoban_train.all_envs[i][0], plan_learned) # save new solution\n",
    "            break\n",
    "    print(plan_learned)\n",
    "\n",
    "print(\"envs improved\", improved, improved / len(sokoban_train.all_envs))\n",
    "\n",
    "with open('data/sokoban_train.pkl', 'wb') as f:\n",
    "    pickle.dump(sokoban_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create learning-data\n",
    "\n",
    "imitation_data = {\n",
    "    'state': [],\n",
    "    'action': [],\n",
    "    'length': [],\n",
    "    'state_val': [],\n",
    "    'action_val': [],\n",
    "    'length_val': [],\n",
    "}\n",
    "\n",
    "print('Validation envs 0 -', split)\n",
    "\n",
    "# walk env solution pathes\n",
    "max_length = 40 # longer solutions are very sparse\n",
    "length_index = [[] for _ in range(max_length)]\n",
    "for i in tqdm(range(len(sokoban_train.all_envs))):\n",
    "    validation = i < split\n",
    "    env, plan, _ = sokoban_train.get(i)\n",
    "    plan_length = len(plan)\n",
    "    for action in plan:\n",
    "        if plan_length <= max_length: # only save plan up to max length\n",
    "            action_hot = np.eye(4)[action]\n",
    "            if validation:\n",
    "                imitation_data['state_val'].append(binary(strip(env))[0])\n",
    "                imitation_data['action_val'].append(action_hot)\n",
    "                imitation_data['length_val'].append(plan_length)\n",
    "            else:\n",
    "                length_index[plan_length - 1].append((strip(env), action_hot))\n",
    "\n",
    "        done = env.step(action_translator[action], 'tiny_rgb_array')[2]\n",
    "        plan_length -= 1\n",
    "    if not done: print(\"solving error\")\n",
    "    else: assert(done and plan_length == 0)\n",
    "\n",
    "# oversample to balance length\n",
    "state_count = len(length_index[0])\n",
    "for i in tqdm(range(max_length)):\n",
    "    for j in range(state_count):\n",
    "        x = j % len(length_index[i])\n",
    "\n",
    "        imitation_data['state'].append(length_index[i][x][0])\n",
    "        imitation_data['action'].append(length_index[i][x][1])\n",
    "        imitation_data['length'].append(i + 1)\n",
    "\n",
    "for i in ['state_val', 'action_val', 'length_val']:\n",
    "    imitation_data[i] = np.array(imitation_data[i])\n",
    "\n",
    "with open('data/imitation_data.pkl', 'wb') as f:\n",
    "    pickle.dump(imitation_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for search\n",
    "\n",
    "with open('data/imitation_data.pkl', 'rb') as f:\n",
    "    imitation_data = pickle.load(f)\n",
    "\n",
    "state = []\n",
    "action = []\n",
    "length = []\n",
    "\n",
    "sample_count = len(imitation_data['state'])\n",
    "for i in range(4 * sample_count):\n",
    "    x = i % sample_count\n",
    "    s = binary(imitation_data['state'][x], shrink=True, random=True, size=(13, 13))[0]\n",
    "    s, a = augment(s, imitation_data['action'][x])[:2]\n",
    "    l = imitation_data['length'][x]\n",
    "    state.append(s)\n",
    "    action.append(a)\n",
    "    length.append(l)\n",
    "\n",
    "state = np.array(state)\n",
    "action = np.array(action)\n",
    "length = np.array(length)\n",
    "\n",
    "\n",
    "# make train samples unique\n",
    "state, indices = np.unique(state, axis=0, return_index=True)\n",
    "print('unique ratio:', len(state) / len(action))\n",
    "action = action[indices]\n",
    "length = length[indices]\n",
    "\n",
    "\n",
    "print('action distribution:', np.sum(action, axis=0) / len(action))\n",
    "print('length distribution:', np.unique(length, return_counts=True)[1])\n",
    "print('length distribution val:', np.unique(imitation_data['length_val'], return_counts=True)[1])\n",
    "\n",
    "\n",
    "file = 'data/imitation_search_{}.npy'\n",
    "np.save(file.format('state'), state)\n",
    "np.save(file.format('action'), action)\n",
    "np.save(file.format('length'), length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot HpBandster search results\n",
    "\n",
    "def capacity(network, shape_out, shape_in=None):\n",
    "    if shape_in is None: shape_in = [13, 13, 4]\n",
    "    c = 0\n",
    "    for l in network[0]: # conv layer\n",
    "        c += ((l[1] ** 2) * shape_in[2] + 1) * l[0]\n",
    "        shape_in[2] = l[0]\n",
    "        if len(l) > 2: # max pooling\n",
    "            shape_in[0] = math.ceil((shape_in[0] + 1) / l[2])\n",
    "            shape_in[1] = math.ceil((shape_in[1] + 1) / l[2])\n",
    "    flat = shape_in[0] * shape_in[1] * shape_in[2]\n",
    "    for l in network[1]: # dense layer\n",
    "        c += l * flat + l\n",
    "        flat = l\n",
    "    c += shape_out * flat + shape_out\n",
    "    return c\n",
    "\n",
    "\n",
    "with open('data/search_action.pkl', 'rb') as f:\n",
    "    search_results = pickle.load(f)\n",
    "\n",
    "    # lowest loss\n",
    "    search_results.sort(key = lambda entry: entry[0])\n",
    "    lowest = search_results[0]\n",
    "    print('Lowest loss:', lowest)\n",
    "\n",
    "    search_results.sort(key = lambda entry: capacity(entry[3], 4)) # order by network capacity\n",
    "\n",
    "    with open('data/search_action.csv', 'w') as c:\n",
    "        c.write('iteration loss acc\\n')\n",
    "        for i in range(len(search_results)):\n",
    "            c.write('{} {} {}\\n'.format(i, search_results[i][1], search_results[i][2]))\n",
    "\n",
    "    x = np.arange(len(search_results))\n",
    "    plt.axvline(search_results.index(lowest), color='grey', lw=3, alpha=0.5)\n",
    "    plt.scatter(x, [search_results[i][1] for i in x], color='red', marker='.', label='val loss')\n",
    "    plt.scatter(x, [search_results[i][2] for i in x], color='green', marker='.', label='val acc')\n",
    "    plt.xlabel('network')\n",
    "    plt.legend()\n",
    "    plt.savefig('data/search_action.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "with open('data/search_length.pkl', 'rb') as f:\n",
    "    search_results = pickle.load(f)\n",
    "\n",
    "    # lowest loss\n",
    "    search_results.sort(key = lambda entry: entry[0])\n",
    "    lowest = search_results[0]\n",
    "    print('Lowest loss:', lowest)\n",
    "\n",
    "    search_results.sort(key = lambda entry: capacity(entry[3], 1)) # order by network capacity\n",
    "    with open('data/search_length.csv', 'w') as c:\n",
    "        c.write('iteration loss\\n')\n",
    "        for i in range(len(search_results)):\n",
    "            c.write('{} {}\\n'.format(i, search_results[i][1]))\n",
    "\n",
    "    x = np.arange(len(search_results))\n",
    "    plt.axvline(search_results.index(lowest), color='grey', lw=3, alpha=0.5)\n",
    "    plt.scatter(x, [search_results[i][1] for i in x], color='red', marker='.', label='val loss')\n",
    "    plt.xlabel('network')\n",
    "    plt.legend()\n",
    "    plt.savefig('data/search_length.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data generators\n",
    "\n",
    "with open('data/imitation_data.pkl', 'rb') as f:\n",
    "    imitation_data = pickle.load(f)\n",
    "\n",
    "train_action_generator = Generator(imitation_data['state'], imitation_data['action'])\n",
    "train_length_generator = Generator(imitation_data['state'], imitation_data['length'])\n",
    "\n",
    "state_val = imitation_data['state_val']\n",
    "action_val = imitation_data['action_val']\n",
    "length_val = imitation_data['length_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train action network\n",
    "\n",
    "# tensorboard\n",
    "logdir = os.path.join(logs_base_dir, \"il-{}\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "models= (([(29, 6), (24, 4), (26, 8), (16, 5), (22, 4, 2), (23, 7)], [14]),\n",
    "         ([(16, 6), (15, 6), (10, 4), (6, 9), (7, 7, 2), (8, 8), (11, 8), (10, 9)], []),\n",
    "         ([(16, 5), (15, 7), (6, 6), (16, 4), (7, 4), (6, 8), (6, 9), (16, 5)], [83]),\n",
    "         ([(20, 6), (14, 4), (32, 6), (15, 5), (31, 7, 2), (9, 4)], [55]),\n",
    "         ([(15, 3), (15, 7, 2), (11, 7, 2), (8, 7, 2), (9, 5)], []),\n",
    "         ([(15, 8), (6, 7), (13, 9), (14, 7, 2), (12, 5), (4, 8)], [219, 59]))\n",
    "model, _ = create(model=models[0])\n",
    "plot(model, 'models/il_action.png')\n",
    "\n",
    "history = model.fit_generator(generator=train_action_generator, validation_data=(state_val, action_val),\n",
    "                              use_multiprocessing=True, workers=2, epochs=30, callbacks=[tensorboard_callback]).history\n",
    "model.save_weights('models/il_action.h5')\n",
    "\n",
    "\n",
    "# save and plot history\n",
    "with open('models/il_action_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "with open('models/il_action_loss.csv', 'w') as f:\n",
    "    f.write('epoch acc acc_val loss loss_val\\n')\n",
    "    for i in range(len(history['loss'])):\n",
    "        f.write('{} {} {} {} {}\\n'.format(i, history['categorical_accuracy'][i],\n",
    "                                          history['val_categorical_accuracy'][i],\n",
    "                                          history['loss'][i], history['val_loss'][i]))\n",
    "\n",
    "x = np.arange(len(history['loss']))\n",
    "plt.plot(x, history['categorical_accuracy'], color='green', label='acc') \n",
    "plt.plot(x, history['val_categorical_accuracy'], color='green', linestyle='dashed', label='val acc')\n",
    "plt.plot(x, history['loss'], color='red', label='loss')\n",
    "plt.plot(x, history['val_loss'], color='red', linestyle='dashed', label='val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('models/il_action_loss.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train length network\n",
    "\n",
    "# tensorboard\n",
    "logdir = os.path.join(logs_base_dir, \"il-{}\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "models = (([(28, 5), (28, 5), (9, 6), (32, 9, 2), (10, 3, 2), (31, 5, 2), (10, 5, 2), (10, 8, 2)], []),\n",
    "          ([(26, 3), (23, 6), (20, 7), (29, 8, 2), (30, 5, 2), (31, 7), (10, 5)], []),\n",
    "          ([(14, 3), (9, 7), (14, 6), (13, 8), (5, 4), (16, 9), (4, 3), (15, 7)], [49]),\n",
    "          ([(8, 3), (16, 9), (10, 7), (10, 3)], [217]),\n",
    "          ([(9, 3), (11, 7), (14, 8), (9, 4), (15, 8), (12, 9), (13, 8)], [42]),\n",
    "          ([(14, 9), (8, 8), (7, 4), (11, 8), (13, 6), (12, 8), (3, 5)], [226]),\n",
    "          ([(11, 7), (11, 9), (5, 4), (9, 7), (8, 8), (8, 8), (1, 9), (4, 7)], [216, 248, 45]))\n",
    "model, _ = create(model=models[1], shape_out=1, activation_out='linear', loss='custom')\n",
    "plot(model, 'models/il_length.png')\n",
    "\n",
    "history = model.fit_generator(generator=train_length_generator, validation_data=(state_val, length_val),\n",
    "                              use_multiprocessing=True, workers=2, epochs=30, callbacks=[tensorboard_callback]).history\n",
    "model.save_weights('models/il_length.h5')\n",
    "\n",
    "\n",
    "# save and plot history\n",
    "with open('models/il_length_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "with open('models/il_length_loss.csv', 'w') as f:\n",
    "    f.write('epoch loss loss_val\\n')\n",
    "    for i in range(len(history['loss'])):\n",
    "        f.write('{} {} {}\\n'.format(i, history['loss'][i], history['val_loss'][i]))\n",
    "\n",
    "x = np.arange(len(history['loss']))\n",
    "plt.plot(x, history['loss'], color='red', label='loss')\n",
    "plt.plot(x, history['val_loss'], color='red', linestyle='dashed', label='val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('models/il_length_loss.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imitation Learning with exploration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAgger = False\n",
    "\n",
    "model_best = -np.inf\n",
    "model, _ = create()\n",
    "model.load_weights('models/il_action_exploration.h5')\n",
    "\n",
    "model_length, _ = create(shape_out=1, activation_out='linear', loss='mean_absolute_error')\n",
    "model_length.load_weights('models/il_length_exploration.h5')\n",
    "\n",
    "stats = []\n",
    "replay = Replay()\n",
    "\n",
    "indizes = np.arange(split, len(sokoban_train.all_envs))\n",
    "for _ in range(4):\n",
    "    np.random.shuffle(indizes)\n",
    "    for r in range(len(indizes)):\n",
    "        # evaluate policy\n",
    "        if r % 200 == 0:\n",
    "            stats.append(evaluate(model, sokoban_train, split))\n",
    "            print(r, stats[-1])\n",
    "            if np.average(stats[-1][1]) >= model_best: # save better model\n",
    "                model.save_weights('models/il_action_exploration.h5')\n",
    "                model_length.save_weights('models/il_length_exploration.h5')\n",
    "                model_best = np.average(stats[-1][1])\n",
    "                print('saved model', model_best)\n",
    "\n",
    "            with open('models/il_action_exploration.pkl', 'wb') as f: # save stats\n",
    "                pickle.dump(stats, f)\n",
    "\n",
    "\n",
    "        # explore\n",
    "        trace_state = []\n",
    "        trace_action = []\n",
    "        total_reward = 0\n",
    "        env, plan, i = sokoban_train.get(indizes[r])\n",
    "        for _ in range(int(len(plan) * 1.5)):\n",
    "            trace_state.append(copy.deepcopy(env))\n",
    "            state = binary(strip(env))[0]\n",
    "\n",
    "            probs = model.predict(np.expand_dims(state, axis=0))[0]\n",
    "            action = np.argmax(np.random.choice(probs, p=probs) == probs) # sample action\n",
    "            trace_action.append(action)\n",
    "\n",
    "            _, reward, done, info = env.step(action_translator[action], 'tiny_rgb_array')\n",
    "            total_reward += Sokoban.reward(reward, info)\n",
    "            if done: break\n",
    "\n",
    "\n",
    "        # create learning data\n",
    "        trace_state.reverse()\n",
    "        trace_action.reverse()\n",
    "        plan_length = 0 if done else None\n",
    "        for e in range(len(trace_state)):\n",
    "            plan = solve(trace_state[e])\n",
    "            if done or plan: # solvable state\n",
    "                if plan_length is None: plan_length = len(plan)\n",
    "                else: plan_length += 1\n",
    "\n",
    "                if done and plan_length < len(plan): # policy solution is better\n",
    "                    replay.add(strip(trace_state[e]), np.eye(4)[trace_action[e]], reward=plan_length)\n",
    "                else: # add ff solution\n",
    "                    env = trace_state[e]\n",
    "                    for action in range(len(plan)):\n",
    "                        replay.add(strip(env), np.eye(4)[plan[action]], reward=len(plan) - action)\n",
    "                        if DAgger: break # only add states from trace\n",
    "                        solved = env.step(action_translator[plan[action]], 'tiny_rgb_array')[2]\n",
    "                    if not solved and not DAgger: print('ERROR ff plan')\n",
    "\n",
    "\n",
    "        # improve policy\n",
    "        if replay.full() > 1/4:\n",
    "            for _ in range(100):\n",
    "                data = replay.sample()\n",
    "                model.train_on_batch(data[0], data[1])\n",
    "                model_length.train_on_batch(data[0], data[3])\n",
    "\n",
    "plot_stats(stats, 'models/il_action_exploration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-train q-values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-generate dqn learning data\n",
    "\n",
    "imitation_qvalues = {\n",
    "    'state': [],\n",
    "    'qvalue': [],\n",
    "    'state_val': [],\n",
    "    'qvalue_val': [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(sokoban_train.all_envs))):\n",
    "    env, plan, _ = sokoban_train.get(i)\n",
    "\n",
    "    for action in plan[:-1]:\n",
    "        q_values = [None] * 4\n",
    "        for a in range(len(q_values)):\n",
    "            env_a = copy.deepcopy(env)\n",
    "            _, reward, done, info = env_a.step(action_translator[a], 'tiny_rgb_array')\n",
    "            q_values[a] = Sokoban.reward(reward, info)\n",
    "\n",
    "            if done: continue\n",
    "\n",
    "            solution = solve(env_a)\n",
    "            if solution:\n",
    "                for x in range(len(solution)):\n",
    "                    _, reward, done, info = env_a.step(action_translator[solution[x]], 'tiny_rgb_array')\n",
    "                    q_values[a] += (Config.discount**(x + 1)) * Sokoban.reward(reward, info)\n",
    "            else: q_values[a] -= 10 # unsolvable punishment\n",
    "\n",
    "\n",
    "        if i < split: # validation data\n",
    "            imitation_qvalues['state_val'].append(binary(strip(env))[0])\n",
    "            imitation_qvalues['qvalue_val'].append(q_values)\n",
    "        else:\n",
    "            imitation_qvalues['state'].append(strip(env))\n",
    "            imitation_qvalues['qvalue'].append(q_values)\n",
    "\n",
    "        env.step(action_translator[action])\n",
    "\n",
    "\n",
    "for i in ['state_val', 'qvalue_val']:\n",
    "    imitation_qvalues[i] = np.array(imitation_qvalues[i])\n",
    "\n",
    "with open('data/imitation_qvalues.pkl', 'wb') as f:\n",
    "    pickle.dump(imitation_qvalues, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('data/imitation_qvalues.pkl', 'rb') as f:\n",
    "    imitation_qvalues = pickle.load(f)\n",
    "\n",
    "train_qvalue_generator = Generator(imitation_qvalues['state'], imitation_qvalues['qvalue'])\n",
    "\n",
    "state_val = imitation_qvalues['state_val']\n",
    "qvalue_val = imitation_qvalues['qvalue_val']\n",
    "\n",
    "\n",
    "# tensorboard\n",
    "logdir = os.path.join(logs_base_dir, \"il-{}\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "# train network\n",
    "model, _ = create(activation_out='linear')\n",
    "\n",
    "history = model.fit_generator(generator=train_qvalue_generator, validation_data=(state_val, qvalue_val),\n",
    "                              use_multiprocessing=True, workers=2, epochs=30, callbacks=[tensorboard_callback]).history\n",
    "model.save_weights('models/il_qaction.h5')\n",
    "\n",
    "\n",
    "# save and plot history\n",
    "with open('models/il_qaction_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "with open('models/il_qaction_loss.csv', 'w') as f:\n",
    "    f.write('epoch loss loss_val\\n')\n",
    "    for i in range(len(history['loss'])):\n",
    "        f.write('{} {} {}\\n'.format(i, history['loss'][i], history['val_loss'][i]))\n",
    "\n",
    "x = np.arange(len(history['loss']))\n",
    "plt.plot(x, history['loss'], color='red', label='loss')\n",
    "plt.plot(x, history['val_loss'], color='red', linestyle='dashed', label='val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('models/il_qaction_loss.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reinforcement Learning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import *\n",
    "from ppo import *\n",
    "from exploration import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DQN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_best = -np.inf\n",
    "agent = DQNAgent()\n",
    "agent.load('models/rl_qaction_dqn.h5')\n",
    "\n",
    "stats = []\n",
    "replay = Replay()\n",
    "\n",
    "indizes = np.arange(split, len(sokoban_train.all_envs))\n",
    "for _ in range(4):\n",
    "    np.random.shuffle(indizes)\n",
    "    for r in range(len(indizes)):\n",
    "        # evaluate policy\n",
    "        if r % 200 == 0:\n",
    "            stats.append(evaluate(agent.Q, sokoban_train, split))\n",
    "            print(r, stats[-1])\n",
    "            if sum(stats[-1][0]) >= agent_best: # save better model\n",
    "                agent.save('models/rl_qaction_dqn.h5')\n",
    "                agent_best = sum(stats[-1][0])\n",
    "                print('saved agent', agent_best)\n",
    "\n",
    "            with open('models/rl_qaction_dqn.pkl', 'wb') as f: # save stats\n",
    "                pickle.dump(stats, f)\n",
    "\n",
    "\n",
    "        # explore\n",
    "        train = replay.full() > 1/4\n",
    "        env, plan, i = sokoban_train.get(indizes[r])\n",
    "        for j in range(5):\n",
    "            total_reward, done, steps, _ = dqn_episode(copy.deepcopy(env), agent, replay,\n",
    "                                                       max_steps=int(len(plan) * 1.5), train=train)\n",
    "\n",
    "            # add ff transitions\n",
    "            if not done and j % 5 == 0: dqn_episode(copy.deepcopy(env), agent, replay, path=plan, train=train)\n",
    "\n",
    "            if done: break\n",
    "\n",
    "plot_stats(stats, 'models/rl_qaction_dqn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PPO</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_best = -np.inf\n",
    "agent = PPOAgent()\n",
    "agent.load('models/il_action_exploration.h5')\n",
    "\n",
    "stats = []\n",
    "replay = Replay(augment=False)\n",
    "\n",
    "indizes = np.arange(split, len(sokoban_train.all_envs))\n",
    "for _ in range(8):\n",
    "    np.random.shuffle(indizes)\n",
    "    for r in range(len(indizes)):\n",
    "        # evaluate policy\n",
    "        if r % 500 == 0:\n",
    "            stats.append(evaluate(agent.actor, sokoban_train, split))\n",
    "            print(r, stats[-1])\n",
    "            if np.average(stats[-1][1]) >= agent_best: # save better model\n",
    "                agent.save('models/rl_action_ppo.h5')\n",
    "                agent_best = np.average(stats[-1][1])\n",
    "                print('saved agent', agent_best)\n",
    "\n",
    "            with open('models/rl_action_ppo.pkl', 'wb') as f: # save stats\n",
    "                pickle.dump(stats, f)\n",
    "\n",
    "\n",
    "        # explore\n",
    "        env, plan, i = sokoban_train.get(indizes[r])\n",
    "        for j in range(5):\n",
    "            total_reward, done, steps = ppo_episode(copy.deepcopy(env), agent, replay, max_steps=int(len(plan) * 1.5))\n",
    "\n",
    "            stats.append((total_reward, done, steps, env.num_boxes, i))\n",
    "            print(r, stats[-1])\n",
    "\n",
    "            if done: break\n",
    "\n",
    "\n",
    "        # update policy\n",
    "        if r > 0 and r % 5 == 0:\n",
    "            for _ in range(10):\n",
    "                data = replay.sample(256)\n",
    "                loss_value, loss_policy = agent.update(data)\n",
    "            replay.reset()\n",
    "\n",
    "plot_stats(stats, 'models/rl_action_ppo.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
